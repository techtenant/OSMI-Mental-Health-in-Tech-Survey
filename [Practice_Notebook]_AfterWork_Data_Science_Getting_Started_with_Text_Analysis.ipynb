{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Practice Notebook] AfterWork Data Science: Getting Started with Text Analysis",
      "provenance": [],
      "collapsed_sections": [
        "A-5biuSRXXQa",
        "10SwZmAU7C9B",
        "cmrIaXoKpZEv",
        "--XlDSUpoQUc",
        "IDDukg6fmi2z",
        "ylOP7Panodyc",
        "kFtj_ioBq2yo",
        "j17Bb64zpLr_",
        "SaB9PC2NpgND",
        "H7qAps2GphDF",
        "_-wRBBMwptXA",
        "0HXc6zkaptXB",
        "2JvTxu8kptXg",
        "lzQBS9ibtWf0",
        "bnnGjyPOtWf3",
        "nQ3H2y98tWgF",
        "PtvUFTPJvNCJ",
        "JoY27aQEvNCM",
        "Iqj0jQLQvNCY",
        "pA2PE93PrVLH",
        "iIXiW-qCrVLR",
        "1JfTmTTQrVLj",
        "8cx-yueqpZwH",
        "0xl2bcKcx4qn",
        "TX3VG2G4q6Li",
        "XSOkRXKwpiBM",
        "mPY0sxLyxzfS",
        "Pl1Hv2GmxzfU",
        "w4oMhQc3xzfx",
        "BGMaaeSdw6oB",
        "mu2oPqN0xFEN",
        "8-VLfpx3xHNR",
        "-QYcpB8AxXkD",
        "WKMdZL8UxZeH",
        "I9u_5fGExbNy",
        "US1UrVINKtQA",
        "T5eXjVkdMYb2",
        "0xIP3WuVMzFD",
        "yGiPKGWLzGu_",
        "Az7s1z__XVo7",
        "kRjNKPwSXaur",
        "zzT3BlO8zGvH",
        "szNQ47v6cSvS",
        "coDykDg3cWKn"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/techtenant/OSMI-Mental-Health-in-Tech-Survey/blob/master/%5BPractice_Notebook%5D_AfterWork_Data_Science_Getting_Started_with_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pohFeClvA2_8"
      },
      "source": [
        "<font color=\"#4b76b7\">To start practicing, you will need to make a copy of it. Go to File > Save a Copy in Drive. You can then use the new copy that will appear in the new tab.</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-5biuSRXXQa"
      },
      "source": [
        "# AfterWork Data Science: Getting Started with Text Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNrz9mqvXlWd"
      },
      "source": [
        "# Text Processing Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10SwZmAU7C9B"
      },
      "source": [
        "## Importing our Essential Libraries and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGFacUJr6BiA"
      },
      "source": [
        "# Importing the required libraries\n",
        "# ---\n",
        "#\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL5wnYjF6Fb0",
        "outputId": "46bbd61a-ada7-4506-9ea1-2aad5366c510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Importing our dataset\n",
        "# ---\n",
        "# We will explore basic text processing techniques using the following dataset \n",
        "# which contains earthquake-related tweets from 25th April 2015 to 5th May 2015 \n",
        "# in 6 Metropolitan cities of Nepal.\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/2Yf17AI\n",
        "# \n",
        "\n",
        "# see entire column content in the dataframe\n",
        "pd.set_option('display.max_columns', None)  \n",
        "\n",
        "# importing our dataset\n",
        "df = pd.read_csv('https://bit.ly/2Yf17AI')\n",
        " \n",
        "# previewing datset\n",
        "df.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>User_ID</th>\n",
              "      <th>Username</th>\n",
              "      <th>Tweet_Date</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tweet_Link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>823</th>\n",
              "      <td>823</td>\n",
              "      <td>592721353553350656</td>\n",
              "      <td>sangeetkayastha</td>\n",
              "      <td>2015-04-27 16:05:57+00:00</td>\n",
              "      <td>#Nepal #NepalEarthquake , last earthquake 10 m...</td>\n",
              "      <td>https://twitter.com/sangeetkayastha/status/592...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>409</th>\n",
              "      <td>409</td>\n",
              "      <td>591952124990201856</td>\n",
              "      <td>jagannathlc</td>\n",
              "      <td>2015-04-25 13:09:19+00:00</td>\n",
              "      <td>How do we get indication of possible tremors a...</td>\n",
              "      <td>https://twitter.com/jagannathlc/status/5919521...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>68</td>\n",
              "      <td>593538370917138432</td>\n",
              "      <td>navizeapp</td>\n",
              "      <td>2015-04-29 22:12:29+00:00</td>\n",
              "      <td>Nepal earthquake: Crowds grow angry as emergen...</td>\n",
              "      <td>https://twitter.com/navizeapp/status/593538370...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1240</th>\n",
              "      <td>1240</td>\n",
              "      <td>591854052310323200</td>\n",
              "      <td>sanjaysharma_</td>\n",
              "      <td>2015-04-25 06:39:36+00:00</td>\n",
              "      <td>@BBCWorld nepal is the center...of that earthq...</td>\n",
              "      <td>https://twitter.com/sanjaysharma_/status/59185...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>79</td>\n",
              "      <td>593381535081889792</td>\n",
              "      <td>everyEarthquake</td>\n",
              "      <td>2015-04-29 11:49:17+00:00</td>\n",
              "      <td>USGS reports a M4.4 #earthquake 18km NNW of Na...</td>\n",
              "      <td>https://twitter.com/everyEarthquake/status/593...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0             User_ID         Username  \\\n",
              "823          823  592721353553350656  sangeetkayastha   \n",
              "409          409  591952124990201856      jagannathlc   \n",
              "68            68  593538370917138432        navizeapp   \n",
              "1240        1240  591854052310323200    sanjaysharma_   \n",
              "79            79  593381535081889792  everyEarthquake   \n",
              "\n",
              "                     Tweet_Date  \\\n",
              "823   2015-04-27 16:05:57+00:00   \n",
              "409   2015-04-25 13:09:19+00:00   \n",
              "68    2015-04-29 22:12:29+00:00   \n",
              "1240  2015-04-25 06:39:36+00:00   \n",
              "79    2015-04-29 11:49:17+00:00   \n",
              "\n",
              "                                                  Tweet  \\\n",
              "823   #Nepal #NepalEarthquake , last earthquake 10 m...   \n",
              "409   How do we get indication of possible tremors a...   \n",
              "68    Nepal earthquake: Crowds grow angry as emergen...   \n",
              "1240  @BBCWorld nepal is the center...of that earthq...   \n",
              "79    USGS reports a M4.4 #earthquake 18km NNW of Na...   \n",
              "\n",
              "                                             Tweet_Link  \n",
              "823   https://twitter.com/sangeetkayastha/status/592...  \n",
              "409   https://twitter.com/jagannathlc/status/5919521...  \n",
              "68    https://twitter.com/navizeapp/status/593538370...  \n",
              "1240  https://twitter.com/sanjaysharma_/status/59185...  \n",
              "79    https://twitter.com/everyEarthquake/status/593...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QUcZfHZM9Va",
        "outputId": "e0deb894-5e79-42d4-8c74-81f20c6e6c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# dropping irrelevant columns\n",
        "df = df.drop(['Unnamed: 0', 'User_ID', 'Username', 'Tweet_Date', 'Tweet_Link'], axis = 1) \n",
        "\n",
        "# renaming column \n",
        "df.columns = ['tweet']\n",
        "\n",
        "# previewing datset\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>Green earthquake alert (Magnitude 4.6M, Depth:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>Felt #earthquake M5.3 strikes 24 km E of #Kath...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>My home destroy by earthquakes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>@rajunepal What's your take on Earthquake fore...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>714</th>\n",
              "      <td>In #Nepal after #earthquake, Unicef estimates ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1005</th>\n",
              "      <td>((ELB)) Post earthquake: Swayambhunath (aka Mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1026</th>\n",
              "      <td>@ChahanaS @setopati @rsnkarki @madhurdev pls u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>I hope this is the first and last massive eart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>another big tremor #kathmandu #earthquake #Nepal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>729</th>\n",
              "      <td>#earthquake Picture speaks itself. #Patanmanga...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  tweet\n",
              "250   Green earthquake alert (Magnitude 4.6M, Depth:...\n",
              "476   Felt #earthquake M5.3 strikes 24 km E of #Kath...\n",
              "38                      My home destroy by earthquakes \n",
              "1017  @rajunepal What's your take on Earthquake fore...\n",
              "714   In #Nepal after #earthquake, Unicef estimates ...\n",
              "1005  ((ELB)) Post earthquake: Swayambhunath (aka Mo...\n",
              "1026  @ChahanaS @setopati @rsnkarki @madhurdev pls u...\n",
              "434   I hope this is the first and last massive eart...\n",
              "287    another big tremor #kathmandu #earthquake #Nepal\n",
              "729   #earthquake Picture speaks itself. #Patanmanga..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmrIaXoKpZEv"
      },
      "source": [
        "## 1. Characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--XlDSUpoQUc"
      },
      "source": [
        "### Basic Processing Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDDukg6fmi2z"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TENiuMfW6x--",
        "outputId": "a7b14a2c-7776-4798-8d04-79b1f8ec30b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Counting the no. characters in our text\n",
        "# ---\n",
        "# \n",
        "df['char_count'] = df.tweet.apply(len)\n",
        "df[['tweet', 'char_count']].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>char_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1091</th>\n",
              "      <td>((Cada Dia)) earthquake burriedd people in Kat...</td>\n",
              "      <td>136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>#earthquake last tweet in a while... no electr...</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>483</th>\n",
              "      <td>4.9 earthquake, 17km ENE of Banepa, Nepal. Apr...</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1171</th>\n",
              "      <td>Three earthquake injured locals brought to Sum...</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1243</th>\n",
              "      <td>Earthquake felt in our Operation theatre</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  tweet  char_count\n",
              "1091  ((Cada Dia)) earthquake burriedd people in Kat...         136\n",
              "214   #earthquake last tweet in a while... no electr...          53\n",
              "483   4.9 earthquake, 17km ENE of Banepa, Nepal. Apr...          99\n",
              "1171  Three earthquake injured locals brought to Sum...          85\n",
              "1243           Earthquake felt in our Operation theatre          40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSjynqcS5A9V"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# Counting the no. of punctuation characters\n",
        "# ---\n",
        "# \n",
        "df['punctuation_count'] = df.tweet.apply(lambda x: len(\"\".join(_ for _ in x if _ in x.split()))) \n",
        "df[['tweet', 'punctuation_count']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqWrvK7cXF3O"
      },
      "source": [
        "# Example 3\n",
        "# ---\n",
        "# Removing Punctuation Characters\n",
        "# ---\n",
        "# A punctuation mark is a mark, such as a full stop, comma, or question mark, \n",
        "# used in writing to separate sentences and their elements and to clarify meaning.\n",
        "# ---\n",
        "#\n",
        "df['tweet'] = df.tweet.str.replace('[^\\w\\s]','')\n",
        "df[['tweet']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvHgTJRnnJKY"
      },
      "source": [
        "# Example 4\n",
        "# ---\n",
        "# Finding Specific Characters i.e. @\n",
        "# ---\n",
        "#\n",
        "df['no_of_ampersats'] = df.tweet.apply(lambda x: len([x for x in x.split() if x.startswith('@')]))\n",
        "df[['tweet','no_of_ampersats']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFkf1GlqnMoi"
      },
      "source": [
        "# Example 5\n",
        "# ---\n",
        "# Replacing Characters with Characters\n",
        "# ---\n",
        "# We will replace # with space ' '\n",
        "# ---\n",
        "# \n",
        "df['tweet_rp_hash'] = df.tweet.str.replace('#',' ')\n",
        "df[['tweet', 'tweet_rp_hash']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhc-lrBHnO2G"
      },
      "source": [
        "# Example 6\n",
        "# ---\n",
        "# Finding Characters i.e. #, and return the index of the character.\n",
        "# ---\n",
        "# \n",
        "df['tweet_fd_hash'] = df.tweet.str.find(\"#\")\n",
        "df[['tweet', 'tweet_fd_hash']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylOP7Panodyc"
      },
      "source": [
        "#### <font color=\"#4b76b7\">Challenge</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjxX95dyDdTC"
      },
      "source": [
        "# Importing the dataset that we will use in our challenges.\n",
        "# This is a dataset sourced from twitter about natural disasters.\n",
        "# ---\n",
        "# Disaster dataset url = https://bit.ly/31cB8Mq\n",
        "# ---\n",
        "#\n",
        "\n",
        "# Importing our dataset\n",
        "df2 = pd.read_csv('https://bit.ly/31cB8Mq')\n",
        "\n",
        "# dropping irrelevant columns\n",
        "df2 = df2.drop(['id', 'keyword', 'location', 'target'], axis = 1) \n",
        "\n",
        "# renaming column \n",
        "df2.columns = ['tweet']\n",
        "\n",
        "# previewing datset\n",
        "df2.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iZHaHEnGUb9"
      },
      "source": [
        "Use the disaster dataset when going through the following challenges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aycCjJxojao"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Remove the punctuation characters from all the tweets.\n",
        "# --- \n",
        "# \n",
        "df2['char_count'] = df.tweet.apply(len)\n",
        "df2[['tweet', 'char_count']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv6UuGLknyNk"
      },
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Question: Find how many # characters there are for each tweet.\n",
        "# --- \n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AIrK6rhnzem"
      },
      "source": [
        "# Challenge 3\n",
        "# ---\n",
        "# Question: Replace the # characters with with space\n",
        "# --- \n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GunkSfBdn0cD"
      },
      "source": [
        "# Challenge 4\n",
        "# ---\n",
        "# Question: Find # from each tweet and return the index of the character\n",
        "# --- \n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFtj_ioBq2yo"
      },
      "source": [
        "## 2. Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j17Bb64zpLr_"
      },
      "source": [
        "### 2.1 Basic Processing Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaB9PC2NpgND"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcrE_N8NpLsC"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Counting Frequency of Words / No. of words in a Corpus/Text/Vocabulary of Words\n",
        "# ---\n",
        "# \n",
        "df['word_count'] = df.tweet.apply(lambda x: len(str(x).split(\" \")))\n",
        "df[['tweet', 'word_count']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN7Rue-hoygu"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# Find Words Starting With or Ending (endswith) With i.e. # \n",
        "# ---\n",
        "#\n",
        "df['words_starting_with'] = df.tweet.apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
        "df[['tweet','words_starting_with']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEE94xCfo2Mb"
      },
      "source": [
        "# Example 3\n",
        "# ---\n",
        "# Check if Words are Made by Digits or Not \n",
        "# ---\n",
        "#\n",
        "df['words_is_digit'] = df.tweet.apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
        "df[['tweet','words_is_digit']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0SgAo_Vo850"
      },
      "source": [
        "# Example 4\n",
        "# ---\n",
        "# Average Word Length\n",
        "# ---\n",
        "#\n",
        "\n",
        "# We create a custom avg_word function to perform average of each record \n",
        "# then apply the function to the df column.\n",
        "def avg_word(sentence):\n",
        "  words = sentence.split()\n",
        "  try:\n",
        "    z = (sum(len(word) for word in words)/len(words))\n",
        "  except ZeroDivisionError:\n",
        "    z = 0 \n",
        "  return z\n",
        "\n",
        "df['avg_word_length'] = df.tweet.apply(lambda x: avg_word(x))\n",
        "df[['tweet','avg_word_length']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyEFZqZMpBCw"
      },
      "source": [
        "# Example 5\n",
        "# ---\n",
        "# Checking if substring is contained in text\n",
        "# ---\n",
        "#\n",
        "df['contains_nepal'] = df.tweet.str.contains('death').any()\n",
        "df[['tweet','contains_nepal']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7qAps2GphDF"
      },
      "source": [
        "#### <font color=\"#4b76b7\">Challenges</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vGUKObvIbGh"
      },
      "source": [
        "Use the disaster dataset when going through the following challenges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7bbg3v1phDH"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Determine whether there are words with digits.\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiQs8zAdpNGb"
      },
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Question: Find words starting with the substring 'Disaster'.\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y95ezeNcpOkh"
      },
      "source": [
        "# Challenge 3\n",
        "# ---\n",
        "# Question: How many words can be found in each tweet?\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZmEFlyMpQAq"
      },
      "source": [
        "# Challenge 4\n",
        "# ---\n",
        "# Question: What is the average word length for each tweet?\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-wRBBMwptXA"
      },
      "source": [
        "### 2.2 Rare and Unique Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HXc6zkaptXB"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFPCw5mcptXC"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Finding Most Frequent Words i.e. Below are top 10\n",
        "# ---\n",
        "#\n",
        "freq = pd.Series(' '.join(df.tweet).split()).value_counts()[:10]\n",
        "freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5SUmIR5ptXM"
      },
      "source": [
        "# Example 3\n",
        "# ---\n",
        "# Finding Rare Words i.e. 100 of them\n",
        "# ---\n",
        "#\n",
        "freq = pd.Series(' '.join(df.tweet).split()).value_counts()[-100:]\n",
        "freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydywwNWPptXP"
      },
      "source": [
        "# Example 4\n",
        "# ---\n",
        "# Removing Rare Words \n",
        "# ---\n",
        "# \n",
        "freq = list(freq.index)\n",
        "df['tweet_rare_mvd'] = df.tweet.apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "df[['tweet', 'tweet_rare_mvd']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JvTxu8kptXg"
      },
      "source": [
        "#### <font color=\"#4b76b7\">Challenges</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1k909owQsyX"
      },
      "source": [
        "Use the disaster dataset when going through the following challenges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnSc9gtSptXh"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Find 50 rare words from the tweets.\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87cL_JPdptXj"
      },
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Question: Get the 20 most frequent words from the tweets.\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YV-JhAuptXm"
      },
      "source": [
        "# Challenge 3\n",
        "# ---\n",
        "# Question: Remove above rare words from the tweets.\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzQBS9ibtWf0"
      },
      "source": [
        "### 2.3 Uppercase, Lowercase and Propercase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnnGjyPOtWf3"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdIWTWTytWf4"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Finding no. of Uppercase Words \n",
        "# ---\n",
        "#\n",
        "df['no_of_uppercase'] = df.tweet.apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
        "df[['tweet','no_of_uppercase']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOHRwzWXtWgA"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# Finding no. of Lowercase Words \n",
        "# ---\n",
        "#\n",
        "df['no_of_lowercase'] = df.tweet.apply(lambda x: len([x for x in x.split() if x.islower()]))\n",
        "df[['tweet','no_of_lowercase']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DLN_I9u4giO"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# Finding no. of Propercase Words \n",
        "# ---\n",
        "#\n",
        "df['no_of_propercase'] = df.tweet.apply(lambda x: len([x for x in x.split() if x.istitle()]))\n",
        "df[['tweet','no_of_propercase']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ3H2y98tWgF"
      },
      "source": [
        "#### <font color=\"#4b76b7\">Challenges</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7OwdQK5QuMH"
      },
      "source": [
        "Use the disaster dataset when going through the following challenges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_EBifxitWgF"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Find the number of uppercase words in each tweet. \n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLuO5IMHtWgK"
      },
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Question: Find the number of lowercase words in each tweet. \n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtvUFTPJvNCJ"
      },
      "source": [
        "### 2.4 Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoY27aQEvNCM"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cft0FDEi7t2A"
      },
      "source": [
        "# For the following examples, we will use the natural language tooklit (nltk) library\n",
        "# ---\n",
        "#\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiYXoUOYvNCN"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Finding Stop Words\n",
        "# ---\n",
        "#\n",
        "\n",
        "# We will first import a list of stopwords in english\n",
        "# ---\n",
        "# \n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "# Let's see all these stops words are\n",
        "# ---\n",
        "#\n",
        "stop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au2et5sX8lqT"
      },
      "source": [
        "# Then let's see the no. of stopwords in the tweets\n",
        "# ---\n",
        "# \n",
        "df['no_of_stopwords'] = df.tweet.apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "df[['tweet','no_of_stopwords']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfMZ7x-UvNCR"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# Removing these Stop Words\n",
        "# ---\n",
        "#\n",
        "df['tweet'] = df.tweet.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "df[['tweet']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQcUMFSr9yXe"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# Let's see if the stop words are there... 0 means we don't have any stopwords.\n",
        "# ---\n",
        "#\n",
        "df['no_of_stopwords'] = df.tweet.apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "df[['tweet','no_of_stopwords']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqj0jQLQvNCY"
      },
      "source": [
        "#### <font color=\"#4b76b7\">Challenges</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFKH15OlRoee"
      },
      "source": [
        "We will use the disaster dataset when going through the following challenges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLXCpQA8vNCZ"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Find the no. of stop words in each tweet.\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9SGWEmzvNCc"
      },
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Question: Remove stop words from each tweet. \n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA2PE93PrVLH"
      },
      "source": [
        "### 2.3 Part of Speech Tags and Sentiment Related Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIXiW-qCrVLR"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE4xlG9duOAk"
      },
      "source": [
        "# First, we will download the punkt and the averaged_perceptron_tagger into our notebook environment. \n",
        "# which will allow us to find the part of speech tags.\n",
        "# ---\n",
        "#\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbFSTO8esx-J"
      },
      "source": [
        "# We create the function to check and get the part of speech tag count of a words in a given sentence\n",
        "pos_dic = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "def pos_check(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_dic[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wlN0R7XrVLR"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Noun Count\n",
        "# ---\n",
        "#\n",
        "df['noun_count'] = df.tweet.apply(lambda x: pos_check(x, 'noun'))\n",
        "df[['tweet','noun_count']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmW4f82ZrVLY"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# Verb Count \n",
        "# ---\n",
        "#\n",
        "df['verb_count'] = df.tweet.apply(lambda x: pos_check(x, 'verb'))\n",
        "df[['tweet','verb_count']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmKGLpmirkb_"
      },
      "source": [
        "# Example 3\n",
        "# ---\n",
        "# Adjective Count\n",
        "# ---\n",
        "#\n",
        "df['adj_count'] = df.tweet.apply(lambda x: pos_check(x, 'adj'))\n",
        "df[['tweet','adj_count']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpRjJ9oxrl7_"
      },
      "source": [
        "# Example 4\n",
        "# ---\n",
        "# Adverb Count \n",
        "# ---\n",
        "#\n",
        "df['adv_count'] = df.tweet.apply(lambda x: pos_check(x, 'adv'))\n",
        "df[['tweet','adv_count']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BArty_OYrtL4"
      },
      "source": [
        "# Example 5\n",
        "# ---\n",
        "# Pronoun Count \n",
        "# ---\n",
        "#\n",
        "df['pron_count'] = df.tweet.apply(lambda x: pos_check(x, 'pron'))\n",
        "df[['tweet','pron_count']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLQOgz2osE25"
      },
      "source": [
        "# Example 6\n",
        "# ---\n",
        "# Polarity\n",
        "# ---\n",
        "#\n",
        "\n",
        "# Function to get polarity of text using the module textblob\n",
        "def get_polarity(text):\n",
        "    try:\n",
        "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
        "        pol = textblob.sentiment.polarity\n",
        "    except:\n",
        "        pol = 0.0\n",
        "    return pol\n",
        "\n",
        "df['polarity'] = df.tweet.apply(get_polarity)\n",
        "df[['tweet', 'polarity']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT9QOQnusFYU"
      },
      "source": [
        "# Example 7\n",
        "# ---\n",
        "# Subjectivity\n",
        "# ---\n",
        "#\n",
        "\n",
        "# Function to get subjectivity of text using the module textblob\n",
        "def get_subjectivity(text):\n",
        "    try:\n",
        "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
        "        subj = textblob.sentiment.subjectivity\n",
        "    except:\n",
        "        subj = 0.0\n",
        "    return subj\n",
        "\n",
        "df['subjectivity'] = df.tweet.apply(get_subjectivity)\n",
        "df[['tweet', 'subjectivity']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JfTmTTQrVLj"
      },
      "source": [
        "#### <font color=\"#4b76b7\">Challenges</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EWeX9EDrVLk"
      },
      "source": [
        "Use the disaster dataset when going through the following challenges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnH3-YEErVLl"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Create new part of speech tag and sentiment related features \n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cx-yueqpZwH"
      },
      "source": [
        "## 3. Text "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xl2bcKcx4qn"
      },
      "source": [
        "### 3.1 Basic Processing Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX3VG2G4q6Li"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7QS9qWUpNqC"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Length of Text (includes spaces)\n",
        "# ---\n",
        "# \n",
        "df['length_of_text'] = df.tweet.str.len()\n",
        "df[['tweet','length_of_text']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd4gVNVpvv0S"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# Lowercasing Text\n",
        "# ---\n",
        "#\n",
        "df['tweet'] = df.tweet.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "df[['tweet']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slm9EZayv3YG"
      },
      "source": [
        "# Example 3\n",
        "# ---\n",
        "# Finding specific words i.e. Words which have more than 5 letters\n",
        "# --- \n",
        "#\n",
        "df['specific_words'] = df.tweet.apply(lambda x: \" \".join(x for x in x.split() if(len(x)>5))) \n",
        "df[['tweet','specific_words']].sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQo6OMiwDOjO"
      },
      "source": [
        "# Example 4\n",
        "# ---\n",
        "# Splitting concatenated words\n",
        "# ---\n",
        "# We can split concatenated words into separate words i.e. 'thathad' to 'that', 'had' \n",
        "# but we first need to install wordninja & textblob libraries which will allows us to do this operation.\n",
        "# --- \n",
        "# \n",
        "!pip3 install wordninja\n",
        "!pip3 install textblob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSuze7fdDYRN"
      },
      "source": [
        "# Importing the required libraries\n",
        "# ---\n",
        "#  \n",
        "import wordninja \n",
        "from textblob import TextBlob\n",
        "\n",
        "# Performing our splitting \n",
        "# \n",
        "df['tweet'] = df.tweet.apply(lambda x: wordninja.split(str(TextBlob(x))))  \n",
        "df['tweet'] = df.tweet.str.join(' ')\n",
        "df[['tweet']].sample(10) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEpWRE0bwFjD"
      },
      "source": [
        "# Example 5\n",
        "# ---\n",
        "# Spelling Correction\n",
        "# ---\n",
        "# Spelling correction helps in identifying multiple copies of words which can later be removed. \n",
        "# ---\n",
        "#\n",
        "\n",
        "# Lets first quickly do some cleaning\n",
        "# ---\n",
        "\n",
        "# Removing punctuantion characters\n",
        "# ---\n",
        "df['tweet'] = df.tweet.str.replace('[^\\w\\s]','') \n",
        "\n",
        "# Removing special characters i.e. # and @\n",
        "# ---  \n",
        "df['tweet'] = df.tweet.str.replace('#','')\n",
        "df['tweet'] = df.tweet.str.replace('@','')\n",
        "\n",
        "# Removing stop words\n",
        "# ---\n",
        "# \n",
        "df['tweet'] = df.tweet.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "\n",
        "# Previewing our dataset\n",
        "# ---\n",
        "#\n",
        "df[['tweet']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icVPma8pM57c"
      },
      "source": [
        "# Let's now do spelling correction\n",
        "# ---\n",
        "#\n",
        "\n",
        "# For demo purposes we only correct first 20 records\n",
        "# NB: If you choose to correct the entire dataset, it will take time and you'll have to wait.\n",
        "# ---\n",
        "#\n",
        "df['corrected_tweet'] = df['tweet'][:20].apply(lambda x: str(TextBlob(x).correct()))\n",
        "df[['tweet', 'corrected_tweet']].head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSOkRXKwpiBM"
      },
      "source": [
        "#### <font color=\"#4b76b7\">Challenges</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gob761z5TsaL"
      },
      "source": [
        "Use the disaster dataset when going through the following challenges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-sys2WwpiBN"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Determine the length of each tweet.\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD0ksHhJwMcD"
      },
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Question: Transform each tweet by lowercasing it.\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzDB9r1wwNlt"
      },
      "source": [
        "# Challenge 3\n",
        "# ---\n",
        "# Question: Find specific words with more than 3 letters. \n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQdYxbEMEpj2"
      },
      "source": [
        "# Challenge 4\n",
        "# ---\n",
        "# Question: Split concatenated words in each tweet. \n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEpOgDeXwOuu"
      },
      "source": [
        "# Challenge 5\n",
        "# ---\n",
        "# Question: Perform spelling correction of the first 100 texts. \n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPY0sxLyxzfS"
      },
      "source": [
        "### 3.2 Part of Speech Tags and Sentiment Related Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl1Hv2GmxzfU"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xesZoRk5xzfV"
      },
      "source": [
        "# First, we will download the punkt and the averaged_perceptron_tagger into our notebook environment. \n",
        "# which will allow us to find the part of speech tags.\n",
        "# ---\n",
        "#\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr9ohQPVxzfZ"
      },
      "source": [
        "# We create the function to check and get the part of speech tag count of a words in a given sentence\n",
        "pos_dic = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "def pos_check(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_dic[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG1xnno6xzfe"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Noun Count\n",
        "# ---\n",
        "#\n",
        "df['noun_count'] = df.tweet.apply(lambda x: pos_check(x, 'noun'))\n",
        "df[['tweet','noun_count']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcz_RdZJxzfh"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# Verb Count \n",
        "# ---\n",
        "#\n",
        "df['verb_count'] = df.tweet.apply(lambda x: pos_check(x, 'verb'))\n",
        "df[['tweet','verb_count']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq5KmVcUxzfj"
      },
      "source": [
        "# Example 3\n",
        "# ---\n",
        "# Adjective Count\n",
        "# ---\n",
        "#\n",
        "df['adj_count'] = df.tweet.apply(lambda x: pos_check(x, 'adj'))\n",
        "df[['tweet','adj_count']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y3b-vIixzfl"
      },
      "source": [
        "# Example 4\n",
        "# ---\n",
        "# Adverb Count \n",
        "# ---\n",
        "#\n",
        "df['adv_count'] = df.tweet.apply(lambda x: pos_check(x, 'adv'))\n",
        "df[['tweet','adv_count']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMZhz04Cxzfo"
      },
      "source": [
        "# Example 5\n",
        "# ---\n",
        "# Pronoun Count \n",
        "# ---\n",
        "#\n",
        "df['pron_count'] = df.tweet.apply(lambda x: pos_check(x, 'pron'))\n",
        "df[['tweet','pron_count']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZPuQ3wtxzfq"
      },
      "source": [
        "# Example 6\n",
        "# ---\n",
        "# Polarity\n",
        "# ---\n",
        "#\n",
        "\n",
        "# Function to get polatiy of text using the module textblob\n",
        "def get_polarity(text):\n",
        "    try:\n",
        "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
        "        pol = textblob.sentiment.polarity\n",
        "    except:\n",
        "        pol = 0.0\n",
        "    return pol\n",
        "\n",
        "df['polarity'] = df.tweet.apply(get_polarity)\n",
        "df[['tweet', 'polarity']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfWbRPs2xzfu"
      },
      "source": [
        "# Example 7\n",
        "# ---\n",
        "# Subjectivity\n",
        "# ---\n",
        "#\n",
        "\n",
        "# Function to get polatiy of text using the module textblob\n",
        "def get_subjectivity(text):\n",
        "    try:\n",
        "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
        "        subj = textblob.sentiment.subjectivity\n",
        "    except:\n",
        "        subj = 0.0\n",
        "    return subj\n",
        "\n",
        "df['subjectivity'] = df.tweet.apply(get_subjectivity)\n",
        "df[['tweet', 'subjectivity']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4oMhQc3xzfx"
      },
      "source": [
        "#### <font color=\"#4b76b7\">Challenges</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm1-An3Pxzfx"
      },
      "source": [
        "Use the disaster dataset when going through the following challenges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrI10Q8Qxzfy"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Create part of speech tags and sentiment related features \n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGMaaeSdw6oB"
      },
      "source": [
        "## 4. Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu2oPqN0xFEN"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZxctIPTxFEO"
      },
      "source": [
        "# Example 1\n",
        "# --- \n",
        "# Tokenization is the process dividing text documents into smaller parts called tokens. \n",
        "# Tokens can be either words, characters, or subwords. These tokens are very useful \n",
        "# for finding such patterns as well as is considered as a base step for stemming and lemmatization. \n",
        "# ---\n",
        "# - The output of word tokenization can be converted to dataframe for better \n",
        "#   text understanding in machine learning applications. \n",
        "# - It can also be provided as input for further text cleaning steps such as punctuation removal,\n",
        "#   numeric character removal or stemming.\n",
        "# ---\n",
        "# \n",
        "\n",
        "# We can transform our third tweet as shown below\n",
        "# ---\n",
        "# \n",
        "words = TextBlob(df.tweet[2]).words\n",
        "print(words) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-VLfpx3xHNR"
      },
      "source": [
        "#### <font color=\"#4b76b7\">Challenges</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV-gRtWBVRTc"
      },
      "source": [
        "Use the disaster dataset when going through the following challenge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZuS6yI4xHNT"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Tokenize your first tweet and return the words.\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QYcpB8AxXkD"
      },
      "source": [
        "## 5. Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKMdZL8UxZeH"
      },
      "source": [
        "#### Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m4TYl0VVzC3"
      },
      "source": [
        "# We will use PorterStemmer from the NLTK library to perform \n",
        "# stemming, so lets import it.\n",
        "# ---\n",
        "# \n",
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOeZ9wFgxZeI"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Stemming\n",
        "# ---\n",
        "# Stemming refers to the removal of suffices, like ing, ly, s, etc.\n",
        "# It cuts either the beginning or end of the word.\n",
        "# We use stemming to categorize the same type of data by its root word.\n",
        "# ---\n",
        "#\n",
        "df['stemming'] = df['tweet'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "df[['tweet', 'stemming']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUbKCUDhWJ5X"
      },
      "source": [
        "# We will use wordnet from the NLTK library to perform \n",
        "# lemmatization, so lets import it. \n",
        "# ---\n",
        "#\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O31e-tnwxZeL"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# Lemmatization\n",
        "# --- \n",
        "# Lemmatization refers to reducing the word to its root form as found in a dictionary. \n",
        "# The stems returned through lemmatization are actual dictionary words and are semantically \n",
        "# complete unlike the words returned by stemmer.\n",
        "# ---\n",
        "# \n",
        "\n",
        "# To perform lemmatization, we use will also import the Word object from the textblob library \n",
        "# and pass it the word that you want to lemmatize and then call the lemmatize method as shown\n",
        "# ---\n",
        "#\n",
        "from textblob import Word\n",
        "\n",
        "\n",
        "# Then perform lematization\n",
        "# ---\n",
        "#\n",
        "df['lemmatization'] = df.tweet.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) \n",
        "df[['tweet', 'lemmatization']].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9u_5fGExbNy"
      },
      "source": [
        "#### <font color=\"#4b76b7\">Challenges</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-GizV5LanPJ"
      },
      "source": [
        "Use the disaster dataset when going through the following challenge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4v3mlInxbNz"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Perform stemming on all the tweets and storing them in a new dataframe column.\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh66y7q4xbN2"
      },
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Question: Perform lemmatization in the given tweets and storing the result in a new dataframe column\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US1UrVINKtQA"
      },
      "source": [
        "## 6. N-grams "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5eXjVkdMYb2"
      },
      "source": [
        "#### Example "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhrlTX-sMYb4"
      },
      "source": [
        "# Example \n",
        "# ---\n",
        "# N-Grams refer to a contiguous sequence of n items from a given sample of text or speech.\n",
        "# These items can be phonemes, syllables, letters, words or base pairs according to the application. \n",
        "# N-grams are used is feature extraction techniques such as in the creation of tf-idf features\n",
        "# ---\n",
        "# Ngrams with N=1 are called unigrams, Bigrams (N=2), Trigrams (N=3), etc.\n",
        "# For instance, for a sentence \"I love eating pasta\", 2grams some 2-grams would be (I love), \n",
        "# (love eating) and (eating pasta).  \n",
        "# --- \n",
        "# \n",
        "from nltk import word_tokenize, ngrams\n",
        "\n",
        "# Word ngrams\n",
        "# ---\n",
        "#\n",
        "list(ngrams(word_tokenize(df['tweet'][0]), 2)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmTYDH1arHlc"
      },
      "source": [
        "# Character ngrams \n",
        "# ---\n",
        "#\n",
        "list(ngrams(df['tweet'][0], 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xIP3WuVMzFD"
      },
      "source": [
        "#### <font color=\"green\">Challenge</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT3y0VntMzFE"
      },
      "source": [
        "# Challenge \n",
        "# ---\n",
        "# Get word and character 3-gram from the 3rd tweet in the disaster dataset.\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGiPKGWLzGu_"
      },
      "source": [
        "## 7. Feature Extraction Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az7s1z__XVo7"
      },
      "source": [
        "#### Example: Creating a Bag of Words (BoW) Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZSuuV0szGvF"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Bag of Words (BoW)\n",
        "# --- \n",
        "# Bag of Words is one of the most commonly used feature extraction techniques with TF-IDF being the other. \n",
        "# This is a method of feature extraction with text data where it is a representation of text \n",
        "# that describes the occurrence of words within a document. By using this technique, we only \n",
        "# keep track of word counts and disregard the grammatical details and the word order. \n",
        "# ---\n",
        "# You can consider using BoW in the following situations:\n",
        "# 1. Building a baseline model which you will later optmize or decide to use other models.\n",
        "# 2. If your dataset is small and context is domain specific.\n",
        "# --- \n",
        "# \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer()\n",
        "bag_of_words = count.fit_transform(df['tweet'])\n",
        "\n",
        "# Show feature matrix / Priviewing the created sparse matrix\n",
        "# ---\n",
        "#\n",
        "bag_of_words.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NjWPRGvXezV"
      },
      "source": [
        "# Get feature names\n",
        "feature_names = count.get_feature_names()\n",
        "\n",
        "# View feature names\n",
        "feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejo85wQlXhMi"
      },
      "source": [
        "# Creating a dataframe to visualise our matrix\n",
        "# ---\n",
        "#\n",
        "pd.DataFrame(bag_of_words.toarray(), columns=feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRjNKPwSXaur"
      },
      "source": [
        "#### Example: Creating a TF-IDF (Term Frequency-Inverse Document Frequency) Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGfyBLP0zPiD"
      },
      "source": [
        "# Example 2a\n",
        "# ---\n",
        "# Term Frequency-Inverse Document Frequency \n",
        "# --- \n",
        "# ---\n",
        "# TF-IDF is another feature extraction technique that we can use to represent text \n",
        "# data in a format that can be consumed by models. When we use TF-IDF, we assume \n",
        "# that high frequency may not able to provide much information gain such as in the case of\n",
        "# Bag of words and on the contrary, rare words contribute more weights to the model. \n",
        "# Let's see how we implement this.\n",
        "# ---\n",
        "# \n",
        "\n",
        "# Importing the TfidfVectorizer which will help us with this process\n",
        "# ---\n",
        "#\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Now creating a Word Level TF-IDF feature\n",
        "# ---\n",
        "# For our parameters:\n",
        "# 1.   max_features: We would want to use 1000 most occurring words as features.\n",
        "# 2.   lowercase: We can convert all characters to lowercase before tokenizing.\n",
        "# 3.   stop_words: We can remove stop words as TfidfVectorizer has ability to remove stop words.\n",
        "# 4.   analyzer='word' We can compute tfidf of word n-grams by setting word as the value of analyzer \n",
        "# 5.   ngram_range=(1, 3):  We then set the parameter ngram_range=(a, b),  where a is the minimum \n",
        "#      and b is the maximum size of ngrams we want to include in our features.\n",
        "# ---\n",
        "#\n",
        "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word', ngram_range=(1,3),  stop_words= 'english')\n",
        "df_tweets_vect = tfidf.fit_transform(df['tweet'])\n",
        "\n",
        "# Show feature matrix / Priviewing the created sparse matrix\n",
        "#\n",
        "df_tweets_vect.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4hPSgr0ZSie"
      },
      "source": [
        "# Show tf-idf feature matrix\n",
        "# ---\n",
        "#\n",
        "tfidf.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4kFBnskZT7x"
      },
      "source": [
        "# Creating data frame to view our matrix\n",
        "# ---\n",
        "#\n",
        "pd.DataFrame(df_tweets_vect.toarray(), columns=tfidf.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTrx0wAqLact"
      },
      "source": [
        "# Example 2b\n",
        "# ---\n",
        "# We can also create a character level TF-IDF feature as shown:\n",
        "# ---\n",
        "# For our parameters:\n",
        "# 1.   max_features: We would want to use 1000 most occurring words as features.\n",
        "# 2.   lowercase: We can convert all characters to lowercase before tokenizing.\n",
        "# 3.   stop_words: We will choose remove stop words because TfidfVectorizer has ability to remove stop words.\n",
        "# 4.   analyzer='word' We compute tfidf of character n-grams by setting word as the value of analyzer \n",
        "# 5.   ngram_range=(1, 3):  We then set the parameter ngram_range=(a, b),  where a is the minimum \n",
        "#      and b is the maximum size of ngrams we want to include in our features.\n",
        "# ---\n",
        "#\n",
        "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='char', ngram_range=(1,3),  stop_words= 'english')\n",
        "df_tweets_vect = tfidf.fit_transform(df['tweet'])\n",
        "df_tweets_vect.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzT3BlO8zGvH"
      },
      "source": [
        "#### <font color=\"#4b76b7\">Challenges</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saxkF3hBcQWC"
      },
      "source": [
        "Use the disaster dataset when going through the following challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szNQ47v6cSvS"
      },
      "source": [
        "#### Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2Io-7EozGvI"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Perform feature extraction using bag of words on the tweets feature.\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coDykDg3cWKn"
      },
      "source": [
        "#### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OVKASCIzGvM"
      },
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Question: Create a character and  a word level tf-idf feature of the disaster tweets.\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}